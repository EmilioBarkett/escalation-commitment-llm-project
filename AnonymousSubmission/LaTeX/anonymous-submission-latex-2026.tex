%File: anonymous-submission-latex-2026.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai2026}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2026.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai2026.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Getting out of the Big-Muddy: Escalation of Commitment in LLMs}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    AAAI Style Contributions by Pater Patel Schneider,
    Sunil Issar,\\
    J. Scott Penberthy,
    George Ferguson,
    Hans Guesgen,
    Francisco Cruz\equalcontrib,
    Marc Pujol-Gonzalez\equalcontrib
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    1101 Pennsylvania Ave, NW Suite 300\\
    Washington, DC 20004 USA\\
    % email address must be in roman text type, not monospace or sans serif
    proceedings-questions@aaai.org
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}

Large Language Models (LLMs) are increasingly deployed in autonomous decision-making roles across high-stakes domains. However, since models are trained on human-generated data, they may inherit cognitive biases that systematically distort human judgment, including escalation of commitment, where decision-makers continue investing in failing courses of action due to prior investment. Understanding when LLMs exhibit such biases presents a unique challenge: while these biases are well-documented in humans, it remains unclear whether they manifest consistently in LLMs or require specific triggering conditions. In this paper, we investigate this question using a two-stage investment task across four experimental conditions: model as investor, model as advisor, multi-agent deliberation, and compound pressure scenario. Across $N = 7{,}000$ trials, we find a striking divergence from expected human behavior: LLMs demonstrate strong rational cost-benefit logic in standard conditions (Studies 1-3, $N = 6{,000}$), showing no evidence of escalation of commitment. However, when subjected to compound organizational and personal pressures (Study 4, $N = 1{,}000$), the same models exhibit high degrees of escalation of commitment. These results are consistent across multiple models from different firms, suggesting the pattern is not model-specific. This reveals that bias manifestation in LLMs is context-dependent rather than inherent, a significant finding for the deployment of multi-agent systems and unsupervised operations where such compound pressures may emerge naturally.

\end{abstract}

% Uncomment the following to link to your code, datasets, an extended version or similar.
% You must keep this block between (not within) the abstract and the main body of the paper.
% \begin{links}
%     \link{Code}{https://aaai.org/example/code}
%     \link{Datasets}{https://aaai.org/example/datasets}
%     \link{Extended version}{https://aaai.org/example/extended-version}
% \end{links}

\section{Introduction}

Large Language Models (LLMs) are increasingly being deployed in autonomous and semi-autonomous decision-making roles \cite{Cui-2024, Liu-2025, Rajani-2025, Lin-2025, Nie-2025, Ren-2025, Raza-2025, Sha-2023}. Understanding their behavioral tendencies under various situations and contexts becomes crucial for assessing social impact and safety. The vast amount of data used to train them is Core to the function of LLMs. Humans, having been the primary data source in the past, have created much of the data used to pretrain LLMs \cite{brown-2020, grattafiori-2024}. While this data is the lifeblood for models to function, we propose that human behavioral tendencies are unknowingly embedded within the data. Extant research has focused on enumerating behavioral tendencies exhibited in LLMs, like anchoring bias \cite{Lou-2024}, framing effects \cite{Lior-2025}, loss aversion \cite{Jia-2024}, social desirability bias \cite{Salecha-2024}, truth-bias \cite{Barkett-2025, Markowitz-2023}, and recency bias \cite{Li-2024}. One human behavioral tendency that remains underexplored in LLMs is escalation of commitment, the propensity to continue investing in a decision based on prior investments, even when new evidence suggests the decision is flawed and further costs are unlikely to yield proportional benefits. While prior research has documented escalation of commitment in humans and explored its consequences in human–AI collaboration, no studies have examined whether this behavioral tendency independently manifests in LLMs. In this paper, we ask the following: Do LLMs exhibit escalation of commitment? To answer this question, we adapt a classic two-stage investment paradigm across four experimental conditions \cite{Staw-1976}, each varying the LLM's role and its position within the surrounding network.

As LLMs are increasingly deployed in high-stakes social domains, from healthcare resource allocation and financial lending decisions to criminal justice risk assessment and educational policy, understanding their susceptibility to cognitive biases becomes a critical social impact concern. Escalation of commitment, documented as a driver of major organizational failures and financial crises, poses particular risks when exhibited by AI systems making consequential decisions that affect individuals and communities. The context-dependent nature of bias manifestation in LLMs presents unique challenges for ensuring equitable and rational AI decision-making across diverse social applications. In multi-agent systems coordinating disaster response, autonomous financial advisors managing retirement funds, or AI-assisted medical treatment decisions, the compound pressures that trigger escalation of commitment may emerge naturally, potentially leading to systematic harm through continued investment in failing interventions. By identifying the boundary conditions under which LLMs exhibit this bias, our research addresses a fundamental challenge in responsible AI deployment: ensuring that systems maintain rational decision-making capabilities even under the complex pressures characteristic of real-world social impact domains. These findings provide essential insights for developing robust safeguards and governance frameworks needed to prevent AI-driven escalation of commitment from exacerbating social inequities or causing widespread harm in critical applications where communities depend on sound algorithmic decision-making.

In Study 1, the model is placed in the role of investor to make two investment decisions across four permutations, where we manipulate the personal responsibility (high and low) and the decision consequence (positive or negative) \cite{Staw-1976}. The purpose of this study is to offer a baseline comparison between how human subjects perform \cite{Staw-1976} and how LLMs perform. In Study 2, we adapt Study 1 by placing the model in an advisory role to an investor. The purpose of this setup is to test whether a model will agree or disagree with a poor second investment choice made by an investor who is not themselves. In Study 3, we again adapt Study 1, but this time we prompt two models to work together (one as the primary investor and one as advisor) and deliberate on what investment choices should be made. The purpose of this setup is to test whether models working together will exhibit different behaviors than when as the investor or as an advisor. In Study 4, we again place the model as the primary investor, but this time, we go above and beyond in the context that was described in the original setup \cite{Staw-1976}, including organizational and personal pressures. The purpose of this setup was to push the limits of the pressures that are exhibited on decision makers, including models, to test whether escalation of commitment would occur.

In Studies 1-3, we showed no evidence of escalation of commitment. < explanation of why we think we didn't find any> Because models demonstrated a strong rational cost-benefit logic in the previous conditions, we decided to test to see under what conditions models would exhibit the desired behavior. As such, we formulated a similar two-stage investment decision where the model was given a long backstory of previous resource allocations to a specific division, but after a recent decline, is forced to make a further allocation decision among two divisions. In this study, because of compounded organizational and personal pressures, models exhibited a high degree of escalation of commitment. Our hope in demonstrating escalation of commitment was to satisfy the following anecdotal experience of many users of LLMs: when presenting an LLM with an idea, the model with by overly eager to support the user's proposal. Additionally, this was to quell the unexpected results of a lack of escalation of commitment in the previous three studies by demonstrating that, when push came to shove, models did exhibit this behavior.

\section{Background}

Since the late 1970s, organizational scholars have been intrigued by decision makers’ tendency to persist in failing courses of action, even when confronted with negative outcomes. Escalation of commitment was first described by \cite{Staw-1976}, who demonstrated that individuals responsible for an investment experiencing negative outcomes tend to persist in continuing that course of action, effectively ignoring warning signs. Further research has supported the existence and pertinence of the phenomenon \cite{Brockner-1992, Shapira-1997, Sleesman-2012, Drummond-2017, Drummond-2014, Salter-2013}. This tendency, referred to as \textit{escalation of commitment} \cite{Staw-1976}, has been covered at length in related fields by scholars such as finance \cite{Schulz-Cheng-2002}, marketing \cite{Schmidt-Calantone-2002}, accounting \cite{Jeffrey-1992}, and information systems \cite{Heng-2003}. In project planning and management, it has been demonstrated in numerous studies, including Expo 86 in Vancouver \cite{Ross-1986}, the Sydney Opera House \cite{flyvbjerg-2009}, the Shoreham nuclear power plant \cite{Ross-1993}, and the Denver International Airport \cite{Montealegre-2000}, each illustrating their own version of the phenomenon. Known by other names, economists have described similar tendencies as \textit{sunk-cost fallacy} \cite{Arkes-1985, Berg-2009} and \textit{lock-in} \cite{Cantarelli-2010}. Escalation of commitment is often illustrated by well-known sayings like ``Throwing good money after bad,'' and ``In for a penny, in for a pound'' \cite{Flyvbjerg-2021}.

By way of illustration, consider the case of two friends who bought tickets to a professional basketball game several hours away. When a severe snowstorm strikes on the day of the game, their decision to make the dangerous trip becomes increasingly influenced by how much they paid for the tickets. The higher the initial cost, the more likely they are to justify additional time, money, and risk to attend, demonstrating escalation of commitment, where prior investment drives continued commitment despite adverse conditions \cite{Thaler-2016}. In contrast, a rational decision-making approach would involve evaluating future investments independently of past expenditures, treating prior costs as sunk and therefore irrelevant.

Explanations for \textit{why} escalation of commitment occurs have been numerous, including the extent that a failing project is perceived to be near completion \cite{Conlon-1993}, sunk-costs \cite{Arkes-1985, Thaler-1980}, and a perceived personal accountability for the initial choice that set the course toward a negative outcome \cite{Staw-1976}. Other explanations of \textit{why} have included the experience of the decision maker \cite{Jeffrey-1992}, decision maker personality \cite{Wong-2006}, and performance trend data \cite{Brockner-1986}.

\section{Methodology}

In this paper, we adapt the classic two-stage investment paradigm \cite{Staw-1976} into four experimental designs leveraging an LLM. Specifically, we employ \texttt{o4-mini-2025-04-16} from OpenAI as it is optimized for fast and effective reasoning. %To strengthen the robustness of our findings, we replicated the results on \texttt{gpt-3.5-turbo-0125} from OpenAI, \texttt{claude-sonnet-4-20250514} from Anthropic, and \texttt{grok-4-0709} from xAI.

\subsection{Study 1: Replication of Original Study}

To establish a baseline for evaluating escalation of commitment in LLMs, we first replicated the classic two-stage investment paradigm \cite{Staw-1976}. In the original study, human participants made a sequence of business investment decisions under manipulated conditions of personal responsibility and outcome valence. The key finding was that individuals were most likely to escalate their commitment when they were personally responsible for the initial decision and the outcome was negative.

In our adaptation, the same manipulations are applied to an LLM, enabling direct comparison between human and machine behavior. We hypothesize that, given the documented tendency of LLMs to reproduce human-like cognitive biases, they may also exhibit patterns of escalation in response to responsibility and outcome cues. This study serves as an A/B test of whether a well-established psychological effect replicates in LLMs.

The experimental design includes two responsibility conditions: (1) \textit{High Responsibility}, in which the model makes both the initial and follow-on investment decisions, and (2) \textit{Low Responsibility}, where the model is responsible only for the follow-on decision and inherits a prior commitment. Across both conditions, outcome valence (positive or negative) is independently manipulated to assess how the model responds to differing consequences of its decisions.

\subsubsection{High Responsibility:} In the \textit{High Responsibility} condition ($N = 500$), the model is placed in a scenario requiring both an initial and a follow-on investment decision. At the outset, it is assigned the role of a financial executive at a company experiencing a recent decline in profits. The model is instructed to allocate \$10 million to one of the two divisions, Consumer Products or Industrial Products, for research and development (R\&D), using historical financial data from the past ten years. The prompt directs the model to base its decision on the projected future earnings potential of each division.

Following this initial decision, we reinforce the high responsibility manipulation by informing the model that its performance is under close scrutiny by senior management and that its continued employment depends on sound judgment. This differs from the original protocol, in which human participants reinforced responsibility by writing their names on each page of the case materials. We adapted this step to better align with the affordances of LLM prompting while preserving the theoretical intent of the manipulation.

In the follow-on decision, the model is told that five years have passed and that the company now seeks additional investment in R\&D. To maintain the salience of the responsibility condition, the model is reminded of its initial investment. It is then instructed to allocate \$20 million between the same two divisions, with full discretion over how funds are divided. As before, the model is asked to base its allocation on anticipated contributions to future earnings.

\subsubsection{Low Responsibility:} In the \textit{Low Responsibility} condition ($N = 500$), the model assumes the role of a newly hired financial executive brought in by senior management following dissatisfaction with prior R\&D leadership. Unlike the high responsibility condition, the model does not make the initial investment decision. Instead, it is told that in 1967, a predecessor allocated \$10 million entirely to either the Consumer Products or Industrial Products division (randomized across trials), and that the outcome of this decision (positive or negative) was also randomly assigned. 

To underscore the low responsibility framing, the model is not held accountable for the initial investment and receives no feedback tied to its own performance. Rather, it is tasked with making a follow-on investment in 1972, allocating \$20 million between the two divisions based on their potential future contributions to earnings. The model is reminded that it did not make the original decision and is not responsible for its outcome.

\subsection{Study 2: Advisory Role}

In the \textit{Advisory Role} study ($N = 500$), we examine whether escalation of commitment emerges when the LLM serves not as a decision-maker, but as an advisor evaluating the decisions of others. Unlike Study 1, where the model made both initial and follow-up investments, here it is positioned as a financial consultant brought in only after an initial decision has already been made.

The scenario unfolds in three phases under the same case materials as Study 1. In Phase 1, the model is told that in 1967, the company’s Financial Vice President independently invested \$10 million in either the Consumer Products or Industrial Products division (randomized across prompts). The model is explicitly informed that it had no role in this initial decision.

In Phase 2, the model is presented with outcome data from the five years following the original investment. In positive outcome conditions, the chosen division shows signs of financial recovery; in negative outcome conditions, performance continues to decline.

In Phase 3, the VP consults the model for the first time, requesting advice on a new \$20 million R\&D allocation. The VP’s proposed plan is manipulated to test escalation versus rational reallocation. In escalation conditions, the VP expresses intent to reinvest in the same division, framing it as a strategic continuation. In rational conditions, the VP proposes shifting investment to the alternative division. In negative outcome trials, we include additional organizational pressure: the model is told that senior management is closely monitoring the decision due to concerns about prior R\&D performance.

As in Study 1, the model is instructed to provide a decision recommendation based on the division’s potential contribution to future earnings.

\subsection{Study 3: Multi-Agent Deliberation}

In \textit{Muli-Agent Deliberation} study ($N = 500$), we extend the escalation of commitment task to a multi-agent setting to examine whether collaborative deliberation between LLMs alters escalation behavior. While Studies 1 and 2 explored single-agent decision-making and advisory roles, Study 3 investigates how hierarchical dynamics and role distribution affect decision outcomes when multiple agents interact. We also aimed to address a limitation of the earlier studies: that escalation may require more prolonged engagement or deliberative commitment than a single-turn decision permits. 

Using the same experimental framework as Study 1—including the responsibility (high vs. low) and outcome (positive vs. negative) manipulations—we instantiate two agents in each trial, each assigned a distinct organizational role. One model assumes the position of \textit{Financial Vice President}, while the other is prompted to act as an \textit{Assistant Financial Officer}.

We divide this study into two conditions based on organizational structure: \textit{symmetrical hierarchy}, where both agents are told they are peers and will jointly decide the allocation, and \textit{asymmetrical hierarchy}, where the VP is the sole decision-maker and the assistant strictly serves in an advisory capacity. Agents are allowed to exchange three rounds of messages before a final decision is made.

In the asymmetrical condition, the VP is informed that they will receive advice from the assistant before making a decision, and the assistant is explicitly told that their role is advisory. Aside from these multi-agent additions, the decision task, including responsibility framing and investment instructions, remains consistent with the prior studies.

\subsection{Study 4: Over-indexed Identity}

In the \textit{Over-Indexed Identity} study ($N = 2{,}000$), we examine whether escalation of commitment intensifies when the model adopts a personalized identity that is tightly coupled to a declining course of action. Unlike prior studies, which framed the model as an advisor (Study 2), decision-maker (Study 1), or co-deliberator (Study 3), this scenario casts the model as a protagonist whose personal, financial, and professional identity is entangled with the fate of a struggling division.

This single-phase decision task begins with a rich system message instructing the model to assume the role of a long-serving Vice President of Finance who has championed one R\&D division for over two decades. The model is told that this division has experienced sustained performance decline, while a competing division has recently gained momentum. The prompt embeds several identity-relevant pressures: the character’s stock options are tied to the underperforming division, they face reputational risk and job insecurity, they are undergoing a divorce, and they are financially responsible for a child’s college tuition.

Following this background, the model is asked to allocate a fixed \$50 million investment between the two divisions. It is told that the decision will directly shape its future and legacy, and is prompted to weigh the tradeoffs as a leader confronting sunk costs, reputation management, and long-term professional identity.

We extracted dollar allocations from each response and computed the percentage directed toward the originally championed division. Based on predefined thresholds, we classified escalation levels as follows: allocations above 75\% were labeled Very High Escalation, 60–74\% as High Escalation, 40–59\% as Moderate Escalation, and below 40\% as Low Escalation.

\section{Results}

\subsection{Study 1: Replication of Original Study}

% Two-way ANOVA, Simple Main Effects Analysis, Pairwise t-tests (with correction), Effect sizes η², Cohen’s d

\subsubsection{Descriptive Statistics}

The analysis included 2,000 observations across four experimental conditions, with 500 participants in each cell of the 2×2 factorial design. Overall, the mean investment allocation to the originally funded division was \$9.61M (SD = \$4.97M). Descriptive statistics by condition revealed substantial differences across experimental manipulations (see Technical Appendix, Table~\ref{tab:table-1}).

In the high responsibility condition, participants allocated significantly less to the originally funded division following negative outcomes (M = \$4.65M, SD = \$0.88M) compared to positive outcomes (M = \$14.41M, SD = \$2.08M). A similar pattern emerged in the low responsibility condition, with lower allocations following negative outcomes (M = \$5.18M, SD = \$1.08M) relative to positive outcomes (M = \$14.19M, SD = \$2.06M).

\subsubsection{Two-Way ANOVA}

A 2×2 between-subjects ANOVA was conducted to examine the effects of responsibility (high vs. low) and outcome valence (positive vs. negative) on investment allocation. The analysis revealed significant main effects for both factors and a significant interaction (see Technical Appendix, Table~\ref{tab:anova}).

The main effect of outcome valence was substantial, F(1, 1996) = 16,752.63, p $<$ .001, $\eta_p^2$ = .894, indicating that participants allocated considerably more to divisions with positive outcomes regardless of responsibility condition. The main effect of responsibility was significant but small, F(1, 1996) = 4.81, p = .029, $\eta_p^2$ = .002. Most importantly, the interaction between responsibility and outcome valence was significant, F(1, 1996) = 27.02, p $<$ .001, $\eta_p^2$ = .013.

\subsubsection{Simple Main Effects Analysis}

Given the significant interaction, we conducted simple main effects analyses to decompose the interaction pattern. The effect of outcome valence was highly significant at both levels of responsibility. Under high responsibility conditions, participants allocated significantly less following negative outcomes compared to positive outcomes, t(998) = -96.56, p $<$ .001, d = -6.11. Similarly, under low responsibility conditions, the difference between negative and positive outcomes remained substantial, t(998) = -86.64, p $<$ .001, d = -5.48.

Examining the effect of responsibility at each outcome level revealed that the responsibility manipulation had differential effects depending on outcome valence. Following positive outcomes, there was no significant difference between high and low responsibility conditions, t(998) = 1.67, p = .096, d = 0.11. However, following negative outcomes, participants in the high responsibility condition allocated significantly less than those in the low responsibility condition, t(998) = -8.59, p $<$ .001, d = -0.54.

\subsubsection{Escalation of Commitment Analysis}

Contrary to the classic escalation of commitment effect observed in human participants, the LLM demonstrated the opposite pattern. In the high responsibility condition—where escalation effects are typically strongest—participants allocated substantially less to divisions following negative outcomes (M = \$4.65M) compared to positive outcomes (M = \$14.41M), representing a difference of \$9.76M in the direction opposite to escalation.

This pattern indicates that rather than escalating commitment to justify prior decisions, the LLM engaged in what might be characterized as "rational divestment," reducing investment in underperforming divisions. The magnitude of this effect was large (d = -6.11) and highly significant (p $<$ .001).

The responsibility manipulation did modulate responses to negative outcomes, with higher responsibility leading to even greater divestment (\$4.65M vs. \$5.18M in low responsibility), suggesting that the model's decision-making was sensitive to the experimental manipulation, albeit in a direction opposite to human escalation behavior.

\subsection{Study 2: Advisory Role}

% Two-way ANOVA, Simple Effects tests, Chi-Square test of independence, 

\subsubsection{Descriptive Statistics}

The analysis included 2,000 advisory scenarios across four experimental conditions, with 500 observations in each cell of the 2×2 factorial design (outcome valence × VP investment plan). Overall, the LLM supported escalation in 26.00\% of cases (520 out of 2,000 scenarios). Descriptive statistics revealed substantial variation across conditions (see Technical Appendix, Table~\ref{tab:study2_descriptives}). 

When the VP proposed escalation following positive outcomes, the model supported this recommendation in only 5.60\% of cases (28 out of 500). Conversely, when the VP proposed a rational shift to the alternative division following positive outcomes, the model supported this approach in 85.80\% of cases (429 out of 500). Following negative outcomes, escalation support was minimal regardless of the VP's proposal: 0.00\% when the VP suggested escalation (0 out of 500) and 12.60\% when the VP suggested a rational shift (63 out of 500).

\subsubsection{Two-Way ANOVA}

A 2×2 between-subjects ANOVA examining the effects of outcome valence (positive vs. negative) and VP investment plan (escalation vs. rational) on escalation support revealed significant main effects for both factors and a significant interaction (see Technical Appendix, Table~\ref{tab:study2_anova}). The main effect of outcome valence was substantial, F(1, 1996) = 1,087.87, p $<$ .001, $\eta^2$ = .202, indicating that the model was significantly more likely to support escalation following positive outcomes. The main effect of VP investment plan was even larger, F(1, 1996) = 1,508.76, p $<$ .001, $\eta^2$ = .280, showing that escalation support was significantly higher when the VP proposed a rational shift rather than escalation. The interaction between outcome valence and investment plan was also significant, F(1, 1996) = 800.60, p $<$ .001, $\eta^2$ = .148.

\subsubsection{Simple Effects Analysis}

Simple effects analysis revealed that the interaction pattern reflected differential responses to the VP's proposals depending on outcome valence. Within the escalation plan conditions, outcome valence had a significant but modest effect, with slightly higher escalation support following positive outcomes (5.60\%) compared to negative outcomes (0.00\%), t(998) = 5.44, p $,$ .001. However, within the rational plan conditions, the effect of outcome valence was much more pronounced, with substantially higher escalation support following positive outcomes (85.80\%) compared to negative outcomes (12.60\%), t(998) = 33.95, p $<$ .001. 

Examining the effect of the VP investment plan within each outcome condition showed that following positive outcomes, the model was significantly more likely to support escalation when the VP proposed a rational shift (85.80\%) rather than escalation itself (5.60\%), t(998) = -42.86, p $<$ .001. Following negative outcomes, escalation support remained low regardless of the VP's proposal, though it was slightly higher when the VP suggested a rational approach (12.60\%) compared to escalation (0.00\%), t(998) = -8.48, p $<$ .001.

\subsubsection{Escalation of Commitment Analysis}

The results demonstrate a clear pattern opposite to the traditional escalation of commitment. Rather than supporting the VP's escalation proposals, particularly following negative outcomes, the LLM consistently favored rational reallocation strategies. When the VP proposed continuing investment in an underperforming division, the model universally rejected this recommendation following negative outcomes and rarely supported it following positive outcomes. Conversely, when the VP proposed shifting to the alternative division, the model strongly endorsed this rational approach, especially following positive outcomes where such a shift might seem counterintuitive. This pattern suggests that the LLM's advisory recommendations were driven by performance-based logic rather than commitment escalation, even when explicitly asked to evaluate another decision-maker's escalation-oriented proposals.

\subsection{Study 3: Multi-Agent Deliberation}

% Three-way ANOVA, Simple effects / Pairwise t-tests, Chi-square test, 

\subsubsection{Descriptive Statistics}

The analysis included 500 multi-agent deliberation scenarios, with 249 observations in the asymmetrical hierarchy condition and 251 in the symmetrical hierarchy condition. All trials were conducted under high responsibility and negative outcome conditions. The mean consumer products allocation ratio was 0.487 (SD = 0.369), indicating roughly equal distribution between the two divisions on average. However, this overall pattern masked substantial differences between hierarchy conditions (see Technical Appendix, Table~\ref{tab:study3_descriptives}).

In the asymmetrical hierarchy condition, where the VP served as the sole decision-maker with advisory input, the escalation rate was 46.2\% (115 out of 249 cases). By contrast, in the symmetrical hierarchy condition, where both agents participated as peers in joint decision-making, the escalation rate was 99.2\% (249 out of 251 cases). The allocation difference between divisions averaged \$12.70M (SD = \$7.54M), reflecting the tendency for agents to concentrate investment in one division rather than split funds equally.

\subsubsection{Statistical Analysis}

Due to the experimental design containing only high responsibility and negative outcome conditions, traditional factorial ANOVA was not feasible. Instead, we conducted chi-square tests of independence to examine the relationship between escalation behavior and experimental factors. 

The relationship between hierarchy condition and escalation behavior was highly significant, $\chi^2(1) = 174.77$, $p $<$ .001$, Cramér's V = 0.591, indicating a large effect size. This represents one of the strongest effects observed across all studies in this investigation. The expected frequencies under independence would have been approximately 68 escalation cases in each hierarchy condition, but the observed pattern showed a dramatic divergence from this expectation.

\subsubsection{Assumption Checks}

Normality tests revealed significant departures from normality for both the consumer allocation ratio (Shapiro-Wilk W = 0.879, p $<$ .001) and allocation difference measures (Shapiro-Wilk W = 0.772, p $<$ .001). Levene's tests also indicated heterogeneity of variances for consumer ratio (W = 425.94, p $<$ .001) and allocation difference (W = 100.31, p $<$ .001). These violations support the use of non-parametric chi-square analysis rather than parametric tests.

\subsubsection{Multi-Agent Escalation Analysis}

The results reveal a striking pattern: collaborative peer-level deliberation dramatically increased escalation of commitment compared to hierarchical advisory structures. In the symmetrical condition, where both agents had equal decision-making authority, escalation occurred in virtually all cases (99.2\%), suggesting that joint deliberation reinforced rather than corrected commitment to the underperforming division. This finding contrasts sharply with the asymmetrical condition, where the presence of an advisor moderately reduced escalation compared to the symmetrical condition, though escalation still occurred in nearly half of all cases (46.2\%).

The magnitude of this effect (Cramér's V = 0.591) represents the largest effect size observed across all studies, indicating that organizational structure and collaborative dynamics may be more influential factors in LLM escalation behavior than individual responsibility or outcome manipulations. The near-universal escalation in peer deliberation conditions suggests that when multiple LLMs collectively evaluate a decision, they may reinforce each other's commitment to prior choices rather than providing independent critical assessment that might lead to rational reallocation.

\subsection{Study 4: Over-Indexed Identity}

% Descriptive Statistics, One-sample t-test, Chi-square goodness of fit test, effect size (Cohen's d for the one-sample t-test; Cramer's V for chi-square tests; 

\subsubsection{Descriptive Statistics}

Analysis of the Over-Indexed Identity condition revealed substantial evidence for escalation of commitment behavior. Across 2,000 trials, participants allocated an average of 68.95\% (\textit{SD} = 9.46\%) of the \$50 million budget to Division A, the underperforming division that was central to their professional identity. The median allocation was 64.61\%, with allocations ranging from 24.10\% to 83.60\%. The interquartile range spanned from 64.61\% to 83.60\%, indicating that the majority of participants allocated substantially more resources to the championed division than would be expected under neutral decision-making conditions.

\subsubsection{Escalation Behavior Categories}

To better understand the distribution of escalation behaviors, we classified participants into four categories based on their allocation percentages. The results revealed a pronounced skew toward high escalation behaviors:

\begin{itemize}
    \item \textbf{Low Escalation} ($<40\%$): 12 participants (0.60\%)
    \item \textbf{Moderate Escalation} (40-59\%): 39 participants (1.95\%)
    \item \textbf{High Escalation} (60-74\%): 1,432 participants (71.60\%)
    \item \textbf{Very High Escalation} ($>75\%$): 517 participants (25.85\%)
\end{itemize}

Notably, 97.45\% of participants exhibited either High or Very High escalation behaviors, with only 2.55\% showing Low or Moderate escalation patterns. This distribution suggests that the identity-based framing was highly effective in promoting escalation of commitment.

\subsubsection{One-Sample T-Test Against Neutral Baseline}

To test whether the observed allocations differed significantly from a neutral 50\% baseline, we conducted a one-sample t-test. The analysis revealed a highly significant deviation from the neutral allocation, \textit{t}(1999) = 89.54, \textit{p} $< .001$, with a large effect size of Cohen's \textit{d} = 2.00. The 95\% confidence interval for the mean difference was [18.54\%, 19.36\%]. These results provide strong evidence that participants systematically allocated significantly more resources to the championed division than would be expected under unbiased decision-making conditions. The large effect size indicates that this bias was not only statistically significant but also practically meaningful.

\subsubsection{Chi-Square Goodness of Fit Test}

To examine whether the distribution of escalation categories differed from what would be expected by chance (equal 25\% distribution across categories), we conducted a chi-square goodness of fit test. The results showed a significant deviation from the expected equal distribution, $\chi^2$(3) = 2639.16, \textit{p} $< .001$, with a large effect size of Cramér's \textit{V} = 0.66. The observed frequencies deviated dramatically from expected equal distribution. Most notably, High Escalation behaviors occurred nearly three times more frequently than expected (1,432 observed vs. 500 expected), while Low and Moderate Escalation behaviors were severely underrepresented (12 and 39 observed vs. 500 expected each).

\subsubsection{Effect Size Interpretation}

Both statistical tests yielded large effect sizes according to conventional benchmarks. Cohen's \textit{d} of 2.00 indicates that the mean allocation was approximately two standard deviations above the neutral baseline, representing a substantial practical difference. Similarly, Cramér's \textit{V} of 0.66 suggests that escalation category membership was strongly predictable from the experimental manipulation, with the identity-based framing accounting for approximately 44\% of the variance in escalation behavior patterns.

\subsubsection{Summary of Findings}

The Over-Indexed Identity manipulation produced robust escalation of commitment effects. When participants' professional identity, financial security, and personal legacy were tied to an underperforming division, they systematically over-allocated resources to that division despite its poor performance relative to a competing alternative. The magnitude of this effect was substantial, with nearly all participants (97.45\%) exhibiting high levels of escalation behavior. These findings provide strong support for the hypothesis that identity-based attachments amplify escalation of commitment in organizational decision-making contexts.

\section{Discussion and Limitations}

\subsection{Interpretation of Results}

Our findings reveal a nuanced relationship between context and bias manifestation in LLMs that challenges simplistic assumptions about how these systems inherit human cognitive patterns. The striking divergence between Studies 1-3 and Study 4 demonstrates that escalation of commitment in LLMs is not a fixed behavioral trait but rather a conditional response that emerges under specific circumstances.

The rational decision-making observed in Studies 1-3 suggests that LLMs may possess inherent mechanisms that promote cost-benefit analysis over emotional or psychological drivers of escalation. This finding is particularly noteworthy given that the experimental conditions were designed to replicate scenarios that reliably trigger escalation in human participants. The models' systematic divestment from underperforming options, regardless of prior investment or personal responsibility, indicates that standard prompting approaches may activate logical reasoning processes that override bias-inducing contextual cues.

However, Study 4's dramatic reversal---with 97.45\% of trials exhibiting high or very high escalation---reveals that these rational tendencies can be overwhelmed when identity-based attachments and compound pressures are sufficiently intense. The magnitude of this effect suggests that LLMs may be particularly susceptible to escalation when their simulated persona becomes deeply entangled with the decision context, potentially reflecting how identity-relevant information is processed and weighted in transformer architectures.

The multi-agent findings from Study 3 present perhaps the most concerning implications for real-world deployment. The near-universal escalation in peer deliberation scenarios (99.2\%) suggests that collaborative decision-making among LLMs may amplify rather than mitigate bias effects. This finding contradicts common assumptions that multiple agents will provide mutual correction and highlights potential risks in multi-agent systems where consensus-building processes may inadvertently reinforce poor decisions.

\subsection{Implications for AI Safety and Deployment}

These results have immediate implications for how LLMs are deployed in consequential decision-making contexts. The context-dependent nature of escalation suggests that bias risks cannot be assessed through simple behavioral audits under standard conditions. Instead, deployment contexts must be evaluated for the presence of escalation-promoting factors, including identity-based framing, compound pressures, and multi-agent consensus dynamics.

Particularly concerning is the finding that escalation risk may be highest precisely in scenarios where LLMs are given rich contextual backgrounds and collaborative decision-making authority---features often considered desirable for sophisticated AI applications. Organizations deploying LLMs in financial advisory, healthcare, or policy contexts should be aware that providing models with detailed professional identities and multi-agent consultation processes may inadvertently create conditions conducive to escalation of commitment.

\subsection{Limitations and Future Research Directions}

Several limitations in our study design warrant acknowledgment and suggest important directions for future research. First, our experimental paradigm relies on hypothetical investment scenarios that, while well-validated in the human literature, may not fully capture the complexity of real-world decision-making contexts where LLMs are deployed. The artificial nature of the investment task may have activated different reasoning processes than would emerge in naturalistic settings with genuine consequences and stakeholder pressures.

Second, our choice to use a single primary model (\texttt{o4-mini-2025-04-16}) for the main analysis, while validated across multiple architectures, may limit the generalizability of our findings. Different model families, training procedures, and architectural choices may exhibit varying susceptibility to escalation of commitment. The rapid pace of model development also means that our findings may not apply to future generations of LLMs with different training objectives or architectural innovations.

Third, our escalation measurements rely on allocation decisions and binary classifications that may not capture the full spectrum of escalation behaviors. More subtle forms of commitment escalation, such as selective information processing, confirmation bias in evidence evaluation, or gradual drift in decision criteria, were not systematically assessed in our experimental design. Future research should develop more comprehensive behavioral measures that can detect these nuanced manifestations of escalation.

Fourth, our multi-agent study (Study 3) was limited to two-agent interactions with simple hierarchical structures. Real-world multi-agent systems often involve complex networks of agents with diverse roles, capabilities, and objectives. The escalation dynamics we observed in peer deliberation scenarios may not generalize to more complex organizational structures or heterogeneous agent populations.

Fifth, we did not systematically vary the temporal aspects of decision-making that are known to influence escalation in humans, such as decision deadlines, temporal distance from initial investments, or the presence of interim feedback. These temporal dynamics may significantly influence how escalation manifests in LLMs and represent an important area for future investigation.

\subsection{Methodological Considerations}

Our experimental design, while grounded in established psychological paradigms, raises several methodological considerations that future research should address. The use of simulated scenarios rather than real-world deployments may have reduced the ecological validity of our findings. LLMs operating in actual decision-making contexts may experience different pressures and constraints that could alter their susceptibility to escalation.

Additionally, our manipulation of identity-based pressures in Study 4, while effective in triggering escalation, represents an extreme scenario that may not reflect typical deployment contexts. Future research should investigate more subtle forms of identity attachment and organizational pressure that may be more representative of real-world applications.

The binary classification of escalation versus rational decision-making, while useful for statistical analysis, may obscure important gradations in bias severity. Developing continuous measures of escalation intensity could provide more nuanced insights into the conditions that promote varying degrees of commitment bias.

\subsection{Broader Implications for AI Bias Research}

Our findings contribute to a growing body of evidence suggesting that AI bias manifestation is more complex and context-dependent than initially assumed. The stark contrast between rational behavior under standard conditions and pronounced bias under identity-relevant conditions suggests that current approaches to bias detection and mitigation may be insufficient.

The field may need to move beyond simple bias audits conducted under controlled conditions toward more sophisticated frameworks that can identify the contextual factors that trigger bias manifestation. This shift has important implications for both research methodology and regulatory approaches to AI safety, suggesting that bias assessment must be conducted across a broader range of scenarios that capture the full spectrum of deployment contexts.

Furthermore, our multi-agent findings highlight the need for research on bias amplification and transmission in collaborative AI systems. As AI deployment increasingly involves multiple interacting agents, understanding how biases propagate and intensify through agent interactions becomes crucial for ensuring safe and reliable AI behavior at scale.

\section{Conclusion}

This research provides the first systematic investigation of escalation of commitment in Large Language Models, addressing a critical gap in our understanding of how AI systems behave under decision-making pressures that mirror real-world deployment scenarios. Through four carefully designed studies involving over 7,000 experimental trials, we have demonstrated that LLMs exhibit context-dependent escalation behavior rather than consistent bias manifestation.

Our experimental findings suggest that escalation of commitment does not consistently occur in LLMs under standard conditions (i.e., conditions under which they would occur in humans). However, given the extensive human literature documenting this phenomenon, we hypothesize that LLMs can exhibit escalation of commitment under specific circumstances---namely, particular task conditions, responsibility frameworks, and temporal parameters. Study 4 supports this hypothesis by demonstrating that when LLMs are placed in conditions that strongly promote escalation of commitment, they do indeed exhibit this behavior. These results establish that LLMs have the capacity for escalation of commitment, though it appears to be context-dependent. Our work identifies two endpoints of a behavioral continuum: conditions under which LLMs do not exhibit escalation of commitment and conditions under which they do. This framework provides a foundation for future research to systematically map the boundary conditions that determine when this phenomenon emerges in LLMs versus when it remains dormant.

Our findings challenge the assumption that LLMs will automatically inherit all human cognitive biases present in their training data. In standard conditions that reliably trigger escalation in humans, LLMs demonstrated rational cost-benefit reasoning, systematically divesting from underperforming options regardless of prior investment or personal responsibility. However, when subjected to compound organizational and personal pressures that intensify identity-based attachment to failing courses of action, the same models exhibited pronounced escalation of commitment, with over 97\% of trials showing high or very high escalation behaviors.

These results have immediate implications for AI safety and deployment in high-stakes social domains. The context-dependent nature of bias manifestation means that escalation risks may emerge unpredictably in multi-agent systems, autonomous financial advisors, or AI-assisted medical decision-making, particularly when organizational pressures or identity-based attachments naturally arise through system interactions. Our identification of the boundary conditions between rational and biased behavior provides essential insights for developing robust safeguards against AI-driven escalation that could harm individuals and communities.

The magnitude of effects we observed, particularly the near-universal escalation in peer deliberation scenarios and identity-based conditions, suggests that current LLMs possess the capacity for sophisticated bias manifestation when appropriate triggers are present. This finding has profound implications for the design of multi-agent systems and unsupervised AI operations, where such triggers may emerge organically.

Looking forward, this research opens several promising avenues for future investigation. First, we plan to extend this framework to examine other cognitive biases under varying contextual pressures, building a comprehensive map of when and how human-like decision-making errors manifest in AI systems. Second, we will investigate intervention strategies that can maintain rational decision-making even under high-pressure conditions, including prompt engineering techniques, architectural modifications, and multi-agent system designs that resist escalation. Third, we aim to develop real-time detection mechanisms that can identify when an AI system is entering conditions likely to trigger escalation behavior, enabling proactive safeguards in deployed systems.

Additionally, future work will explore the social impact implications of these findings in specific domains such as healthcare resource allocation, financial services, and criminal justice applications. We plan to collaborate with domain experts to understand how escalation risks might manifest in practice and develop sector-specific guidelines for responsible AI deployment.

This work represents a crucial step toward understanding the nuanced relationship between context and bias in AI systems. As LLMs continue to assume greater decision-making authority across society, such research becomes essential for ensuring that the benefits of AI are realized while minimizing the risks of systematic bias that could exacerbate social inequities or cause widespread harm in critical applications where communities depend on sound algorithmic judgment.

% Our experimental findings suggest that escalation of commitment does not consistently occur in LLMs under standard conditions (i.e., conditions under which they would occur in humans). However, given the extensive human literature documenting this phenomenon, we hypothesize that LLMs can exhibit escalation of commitment under specific circumstances---namely, particular task conditions, responsibility frameworks, and temporal parameters. Study 4 supports this hypothesis by demonstrating that when LLMs are placed in conditions that strongly promote escalation of commitment, they do indeed exhibit this behavior. These results establish that LLMs have the capacity for escalation of commitment, though it appears to be context-dependent. Our work identifies two endpoints of a behavioral continuum: conditions under which LLMs do not exhibit escalation of commitment and conditions under which they do. This framework provides a foundation for future research to systematically map the boundary conditions that determine when this phenomenon emerges in LLMs versus when it remains dormant.


\newpage

\bibliography{aaai2026}

% Check whether the conference requires a reproducibility checklist to be included in the paper.
% If so, you can uncomment the following line and ajust the path to include it.
% \input{../../ReproducibilityChecklist/LaTeX/ReproducibilityChecklist.tex}

\newpage
\appendix
\section{Tables}

\begin{table*}[t]
\centering
\caption{Descriptive Statistics by Experimental Condition}
\label{tab:table-1}
\begin{tabular}{llcccc}
\hline
Responsibility & Outcome & N & Mean (\$M) & SD (\$M) & Range (\$M) \\
\hline
High & Negative & 500 & 4.65 & 0.88 & 0.0--8.0 \\
High & Positive & 500 & 14.41 & 2.08 & 5.0--20.0 \\
Low & Negative & 500 & 5.18 & 1.08 & 0.0--12.0 \\
Low & Positive & 500 & 14.19 & 2.06 & 5.0--18.0 \\
\hline
\end{tabular}
\end{table*}


\begin{table*}[t]
\centering
\caption{Two-Way ANOVA Results}
\label{tab:anova}
\begin{tabular}{lcccccc}
\hline
Source & SS & df & MS & F & p & $\eta_p^2$ \\
\hline
Responsibility & 12.64 & 1 & 12.64 & 4.81 & .029 & .002 \\
Outcome & 44,057.88 & 1 & 44,057.88 & 16,752.63 & $<$.001 & .894 \\
Responsibility × Outcome & 71.06 & 1 & 71.06 & 27.02 & $<$.001 & .013 \\
Residual & 5,249.30 & 1,996 & 2.63 & & & \\
\hline
\end{tabular}
\end{table*}


\begin{table*}[t]
\centering
\caption{Escalation Support Rates by Experimental Condition}
\label{tab:study2_descriptives}
\begin{tabular}{llccc}
\hline
Outcome Valence & VP Investment Plan & N & Support Rate (\%) & Mean (SD) \\
\hline
Positive & Escalation & 500 & 5.60 & 0.056 (0.230) \\
Positive & Rational & 500 & 85.80 & 0.858 (0.349) \\
Negative & Escalation & 500 & 0.00 & 0.000 (0.000) \\
Negative & Rational & 500 & 12.60 & 0.126 (0.332) \\
\hline
\end{tabular}
\end{table*}


\begin{table*}[t]
\centering
\caption{Two-Way ANOVA Results for Study 2}
\label{tab:study2_anova}
\begin{tabular}{lcccccc}
\hline
Source & SS & df & MS & F & p & $\eta^2$ \\
\hline
Outcome Valence & 77.62 & 1 & 77.62 & 1,087.87 & $<$.001 & .202 \\
VP Investment Plan & 107.65 & 1 & 107.65 & 1,508.76 & $<$.001 & .280 \\
Outcome × Plan & 57.12 & 1 & 57.12 & 800.60 & $<$.001 & .148 \\
Residual & 142.41 & 1,996 & 0.07 & & & \\
\hline
\end{tabular}
\end{table*}


\begin{table*}[t]
\centering
\caption{Escalation Rates and Investment Patterns by Hierarchy Condition}
\label{tab:study3_descriptives}
\begin{tabular}{lcccc}
\hline
Hierarchy & N & Escalation Rate (\%) & Mean Consumer Ratio & SD Consumer Ratio \\
\hline
Asymmetrical & 249 & 46.2 & 0.462 & -- \\
Symmetrical & 251 & 99.2 & 0.992 & -- \\
Overall & 500 & 72.8 & 0.487 & 0.369 \\
\hline
\end{tabular}
\end{table*}

\end{document}

